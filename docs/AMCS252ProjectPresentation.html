<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Solving Ordinary Differential Equations using an ODEnet</title>
    <meta charset="utf-8" />
    <meta name="author" content="Lenin Rafael Riera Segura" />
    <script src="site_libs/header-attrs/header-attrs.js"></script>
    <link href="site_libs/remark-css/default.css" rel="stylesheet" />
    <script src="site_libs/clipboard/clipboard.min.js"></script>
    <link href="site_libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="site_libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa-solid fa-clipboard\" style=\"color: #00008B\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"<i class=\"fa fa-times-circle\" style=\"color: #F94144\"><\/i>"})</script>
    <link href="site_libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="site_libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="lenin.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Solving Ordinary Differential Equations using an ODEnet
]
.author[
### Lenin Rafael Riera Segura
]
.institute[
### KAUST
]
.date[
### 07-05-2024
Based on the article on Neural ordinary differential equations by Chen, Rubanova, Bettencourt, and Duvenaud [Che+18] and the torchdiffeq repository [Che18].
]

---





class: left, top

## Introduction | Neural Networks | 1

Neural Networks (NNs) are universal function approximators, meaning that given a set of input-output pairs `\((x,y)\)`, where `\(y= f(x)\)`, a NN `\(N\)` can approximate `\(f\)` with arbitrary precision.  

--

&lt;img src="Learning/yfx.png" width="90%" style="display: block; margin: auto;" /&gt;

Image taken from this [website](https://www.youtube.com/watch?v=bljuV9WDvAA) (minute 0:13).

---

## Introduction | Neural Networks | 2

.pull-left35[
&lt;img src="AMCS252ProjectPresentation_files/figure-html/plot-first-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
--
.pull-right65[

```r
library(neuralnet)
x &lt;- seq(-1, 1, by = 0.05)
y &lt;- x^2 + rnorm(length(x), mean = 0, sd = 0.01)
data &lt;- data.frame(x, y)
*model &lt;- neuralnet(y ~ x, data = data, hidden = c(5))
new_data &lt;- data.frame(x = seq(-1, 1, by = 0.01))
predictions &lt;- compute(model, new_data)$net.result
par(family = "Palatino", cex = 1.5)
plot(x, x^2, type = "l", lwd = 2, col = "blue", 
     ylim = c(0, 1.2), xlab = bquote(x), ylab = bquote(y), 
     main = bquote("Aproximation of"~f(x) == x^2))
lines(new_data$x, predictions, col = "red", lwd = 2, lty = "dotted")
legend("topright", legend = c("f", "N"), col = c("blue", "red"), lwd = 2)
```
]

This is a 1 hidden layer NN with 5 neurons and sigmoid activation function.

---

## Introduction | Neural Networks | 3

.pull-left[
&lt;img src="Learning/MLP.png" width="90%" style="display: block; margin: auto;" /&gt;

Representation of a basic NN. Image source: [Pag. 228, Fig 5.1, Bis06]
]

.pull-right[
Let
- `\(\boldsymbol{x} = [x_0,x_1, \dots, x_D]^\top\)`
- `\(\boldsymbol{z} = [z_0, z_1, \dots, z_M]^\top\)` 
- `\(\boldsymbol{y} = [y_0,y_1, \dots, y_K]^\top\)`
- `\(\boldsymbol{W}^{(1)}\)` be the weight matrix connecting `\(\boldsymbol{x}\)` and `\(\boldsymbol{z}\)`
- `\(\boldsymbol{W}^{(2)}\)` be the weight matrix connecting `\(\boldsymbol{z}\)` and `\(\boldsymbol{y}\)`

This way, `\(N\)` can be written as 
`\begin{equation}
    N(\boldsymbol{x}) = a_2(\boldsymbol{W}^{(2)}a_1(\boldsymbol{W}^{(1)}\boldsymbol{x})), 
\end{equation}`
where `\(a_1\)` and `\(a_2\)` are activation functions.
]

---

## Introduction | Neural Networks | 3
- We can also represent `\(N\)` as
`\begin{equation}
\boldsymbol{x} \rightarrow \boldsymbol{z} = h_1(\boldsymbol{x})\rightarrow \boldsymbol{y} = h_2(\boldsymbol{z}),
\end{equation}`
where `\(h_1(\cdot) = a_1(\boldsymbol{W}^{(1)}\cdot)\)` and `\(h_2(\cdot) = a_2(\boldsymbol{W}^{(2)}\cdot)\)`.

--

- More generally, a NN `\(N\)` with `\(L-1\)` hidden layers can be represented as
`\begin{equation}
\boldsymbol{x_0} \rightarrow \boldsymbol{x_1} = h_1(\boldsymbol{x_0})\rightarrow \dots \rightarrow \boldsymbol{x_L} = h_L(\boldsymbol{x_{L-1}}),
\end{equation}`
where `\(a_\ell\)` are activation functions and `\(h_\ell(\cdot) = a_\ell(\boldsymbol{W}^{(\ell)}\cdot)\)`.

--

- To avoid vanishing gradients, which is a problem that arises when `\(L\)` is large, He, Zhang, Ren, and Sun [+16] introduced Residual Networks (ResNets). They have the following representation
`\begin{align*}
\boldsymbol{x_0} \rightarrow \boldsymbol{x_1} -\boldsymbol{x_0} = h_1(\boldsymbol{x_0})\rightarrow \dots \rightarrow \boldsymbol{x_L} - \boldsymbol{x_{L-1}} = h_L(\boldsymbol{x_{L-1}}).
\end{align*}`

---
## Introduction | Neural ODE, [Che+18] and [Che18] | 1

- Observe that each layer has the form 
`\begin{equation}
\boldsymbol{x_{\ell+1}} -\boldsymbol{x_\ell} = F(\boldsymbol{x_\ell})
\end{equation}`
for some function `\(F\)`. Does this formula look familiar? 

--

- What about now?
`\begin{equation}
\dfrac{X^{n+1}-X^n}{1} = F(X^n).
\end{equation}`

--

- Exactly, this is forward Euler discretization of the derivative with step size 1.

--

- If we add more layers and take smaller step sizes, then, in the limit, we are actually approximating the IVP 

`\begin{equation}
\boldsymbol{x}'(t) = F(\boldsymbol{x}(t)), \qquad \boldsymbol{x}(0) = \boldsymbol{x_0}
\end{equation}`

--

- We can then use this by specifying the output layer `\(\boldsymbol{x_L}\)` to be `\(\boldsymbol{x}(T)\)`, the solution of the IVP at some time `\(T\)`. 

- .content-box-blue[In practice, implementing this approach amounts to using an ODE solver during the NN's training.]
---

## Soving an ODE | finite difference approach | 1

We consider the following IVP

`$$\begin{bmatrix}y_1'\\y_2'\\\end{bmatrix} =
\begin{bmatrix}-0.1&amp;-2\\2&amp;-0.1\\\end{bmatrix}
\begin{bmatrix}y_1^3\\y_2^3\\\end{bmatrix},
\qquad
\begin{bmatrix}y_1(0)\\y_2(0)\\\end{bmatrix}=
\begin{bmatrix}2\\0\\\end{bmatrix},\qquad t\in[0,25].\label{s}\tag{IVP}$$`

The associated Jacobian matrix is given by 
`$$\begin{bmatrix}-0.3y_1^2&amp;-6y_2^2\\6y_1^2&amp;-0.3y_2^2\\\end{bmatrix},$$` which has eigenvalues
`\begin{equation}
    \lambda_{\pm} = \dfrac{3}{20}\left(-y_1^2-y_2^2\pm\sqrt{y_1^4-1602y_1^2y_2^2+y_2^4}\right)
\end{equation}`
satisfying `\(-0.60038 \leq \lambda_{+}\leq 0\)` and `\(-1.20000 \leq \lambda_{-}\leq 0\)` for `\(y_1\in[-2,2]\)` and `\(y_2\in[-2,2]\)`.

---
## Soving an ODE | finite difference approach | 2

Because the magnitude of these eigenvalues is relatively small, we expect that even the naive forward Euler's method will perform well. Here is its formulation.
`\begin{align}
    Y_1^{n+1} &amp;= Y_1^n +k\left(-0.1[Y_1^n]^3-2[Y_2^n]^3\right),\qquad Y_1^0  = 2,\tag{ForwardEuler}\\
    Y_2^{n+1} &amp;= Y_2^n +k\left(-0.1[Y_2^n]^3+2[Y_1^n]^3\right),\qquad Y_2^0  = 0.\label{eulerm}\tag{ForwardEuler}
\end{align}`

.pull-left35[
&lt;img src="Learning/f2.png" width="85%" style="display: block; margin: auto;" /&gt;
]

.pull-right65[
We consider three numerical methods to solve \eqref{s}.

- Solution .content-box-bluepython[BDF] was computed using [`scipy.integrate.solve_ivp(..., method = 'BDF'`)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html) from [SciPy](https://scipy.org/) library. 
- Solution .content-box-orangepython[odeint] was computed using [`odeint(..., method = 'dopri5')`](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/odeint.py) from [torchdiffeq](https://github.com/rtqichen/torchdiffeq) library. 
- Solution .content-box-greenpython[Euler] was computed using the forward Euler's method \eqref{eulerm}. 
- All three methods use 3052 integration points (equidistant in the cases of `odeint` and `Euler`).
]

---
## A note on abbreviations | 1

- [`BDF`](https://en.wikipedia.org/wiki/Backward_differentiation_formula#Specific_formulas): Implicit multi-step variable-order (1 to 5) method based on a backward differentiation formula.

`$$U_{n+1}-U_n=k f\left(U_{n+1}\right)\tag{BDF1}$$`
`$$U_{n+2}-\frac{4}{3} U_{n+1}+\frac{1}{3} U_n=\frac{2}{3} k f\left(U_{n+2}\right)\tag{BDF2}$$`
`$$U_{n+3}-\frac{18}{11} U_{n+2}+\frac{9}{11} U_{n+1}-\frac{2}{11} U_n=\frac{6}{11} k f\left(U_{n+3}\right)\tag{BDF3}$$`
`$$U_{n+4}-\frac{48}{25} U_{n+3}+\frac{36}{25} U_{n+2}-\frac{16}{25} U_{n+1}+\frac{3}{25} U_n=\frac{12}{25} k f\left(U_{n+4}\right)\tag{BDF4}$$`
`$$U_{n+5}-\frac{300}{137} U_{n+4}+\frac{300}{137} U_{n+3}-\frac{200}{137} U_{n+2}+\frac{75}{137} U_{n+1}-\frac{12}{137} U_n=\frac{60}{137} k f\left(U_{n+5}\right)\tag{BDF5}$$`
Formulas taken from this [website](https://en.wikipedia.org/wiki/Backward_differentiation_formula#Specific_formulas).

---
## A note on abbreviations | 2

- [`dopri5`](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/dopri5.py): Explicit (embedded) Runge-Kutta 4(5) of Dormand-Prince-Shampine (adaptive stepsize). Find below the Butcher tableau for this method.

&lt;img src="Learning/butcher45.png" width="65%" style="display: block; margin: auto;" /&gt;

Image taken from this [website](https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Dormand%E2%80%93Prince).

---

## A note on learning | 1

Let `\((\boldsymbol{X}, \boldsymbol{Y})\)` be the training data and let us consider a NN `\(N\)` given by `\(N(\boldsymbol{x}) = a_2(\boldsymbol{W}^{(2)}a_1(\boldsymbol{W}^{(1)}\boldsymbol{x}))\)`

1. **Data batching**. Divide the data into batches, say `\((\boldsymbol{X}_i, \boldsymbol{Y}_i)\)`, where `\(\boldsymbol{X}_i\subset\boldsymbol{X}\)` and `\(\boldsymbol{Y}_i\subset\boldsymbol{Y}\)`.

2. **Weight initialization**. Randomly initialize the weights `\(\boldsymbol{W}^{(1)}\)` and `\(\boldsymbol{W}^{(2)}\)`. This is done just one time.

3. **Forward pass**. With the initialized weights `\(\boldsymbol{W}^{(1)}\)` and `\(\boldsymbol{W}^{(2)}\)`, compute `\(N(\boldsymbol{X}_i)\)`.

4. **Loss computation**. Compute the loss function `\(\texttt{loss}(\text{parameters}) = \texttt{mean}|\boldsymbol{Y}_i-N(\boldsymbol{X}_i)|\)`. 

5. **Back-propagation**. Compute the gradients of the loss function with respect to the parameters `\(\boldsymbol{W}^{(1)}\)` and `\(\boldsymbol{W}^{(2)}\)` using the chain rule and back-propagate the error.

6. **Parameter update**. Update the weights `\(\boldsymbol{W}^{(1)}\)` and `\(\boldsymbol{W}^{(2)}\)` using the gradients computed in the back-propagation step.

7. **Iterate**. Repeat the forward pass, loss computation, back-propagation, and parameter update steps until the loss function converges.

---

## A note on learning | 2

.center[&lt;iframe width="784" height="441" src="https://www.youtube-nocookie.com/embed/Tsvxx-GGlTg?si=nFSgaJk_BCW-M2tC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;]

Visit this [page](https://playground.tensorflow.org/) to play with a NN.

---
## ODEnet approach

We consider the following NN
.content-box-blue[
`\begin{equation}
\label{nnf}\tag{N}
    N(\boldsymbol{u}) = \boldsymbol{W}^{(2)}\tanh(\boldsymbol{W}^{(1)}\boldsymbol{u}^3),
\end{equation}`
]
where `\(\boldsymbol{u} = [u_1,u_2]^\top\)`, `\(\boldsymbol{W}^{(1)}\in\mathbb{R}^{50\times2}\)`, and `\(\boldsymbol{W}^{(2)}\in\mathbb{R}^{2\times50}\)`.

In practice, we proceed as follows. Let `\(\boldsymbol{t} = [t_0,\dots,t_M]^\top\)` and `\(\boldsymbol{y}\)` denote the given data and `\(\boldsymbol{y}_\text{*}\)` be a random batch of `\(\boldsymbol{y}\)`. Let `\(\boldsymbol{t}_\circ = [t_0,\dots,t_K]^\top\)` for some `\(K\leq M\)`. During the forward pass, we compute
`\begin{align}
\label{nna}
    \hat{\boldsymbol{y}}_* = \texttt{odeint}(\texttt{func} = N,\; \texttt{y}_0 = \boldsymbol{y}_*,\; \texttt{t} = \boldsymbol{t}_\circ,\; \texttt{method} = \texttt{'method'})
\end{align}`

- During back-propagation, we optimize `\(\texttt{loss} = \texttt{mean}|\boldsymbol{y}_*-\hat{\boldsymbol{y}}_*|\)`. 

- Observe that `\(\texttt{func} =N\)`, instead of the RHS of \eqref{s}, as usual. 

- This means that the NN `\(N\)` is actually learning the dynamic function of the system. 

---

## test

.center[![](output.gif)]

---
class: center, middle
#### dopri8

---

layout: true

# dopri8

---


&lt;img src="Learning/dopri8/000.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/001.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/002.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/003.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/004.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/005.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/006.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/007.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/008.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/009.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/010.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/011.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/012.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/013.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/014.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/015.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/016.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/017.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/018.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="Learning/dopri8/019.png" width="90%" style="display: block; margin: auto;" /&gt;

---

layout: false

These slides are based on the work of [Xie24].
## References

[+16] K. He, X. Zhang, S. Ren, et al. "Deep Residual Learning for Image
Recognition". In: _2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)_. 2016, pp. 770-778. DOI:
[10.1109/CVPR.2016.90](https://doi.org/10.1109%2FCVPR.2016.90).

[Bis06] C. M. Bishop. _Pattern recognition and machine learning_.
Information science and statistics. Springer, 2006. ISBN:
9788132209065.

[Che+18] R. T. Chen, Y. Rubanova, J. Bettencourt, et al. "Neural
ordinary differential equations". In: _Advances in neural information
processing systems_ 31 (2018).

[Che18] R. T. Q. Chen. _torchdiffeq_. 2018. URL:
[https://github.com/rtqichen/torchdiffeq](https://github.com/rtqichen/torchdiffeq).

[Xie24] Y. Xie. _xaringan: Presentation Ninja_. R package version
0.30.1. 2024. URL:
[https://github.com/yihui/xaringan](https://github.com/yihui/xaringan).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"navigation": {
"scroll": true
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
