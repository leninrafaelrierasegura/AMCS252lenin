---
title: "Solving Ordinary Differential Equations using an ODEnet"
subtitle: ""
author: "Lenin Rafael Riera Segura"
institute: "KAUST"
date: "`r format(Sys.time(), '%d-%m-%Y')`\n Based on the article on Neural ordinary differential equations by Chen, Rubanova, Bettencourt, and Duvenaud [Che+18] and the torchdiffeq repository [Che18]."
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      slideNumberFormat: '%current%'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      #countdown: 60000
      navigation:
        scroll: true # disable slide transitions by scrolling
    css: 
      - default
      - xaringan-themer.css
      - lenin.css
mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
---
```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("./ref.bib", check = FALSE)
```

```{r setup, echo = FALSE}
options(htmltools.dir.version = FALSE)
library(xaringanthemer)
style_mono_accent(base_color = "#005EB8")
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa-solid fa-clipboard\" style=\"color: #00008B\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
knitr::opts_chunk$set(
  message = FALSE,    
  warning = FALSE,    
  echo = FALSE,        
  include = TRUE,     
  eval = TRUE,       
  cache = FALSE,       
  fig.align = "center",
  out.width = "90%",
  dpi = 300,
  error = TRUE,
  collapse = TRUE
)
```

class: left, top

## Introduction | Neural Networks | 1

Neural Networks (NNs) are universal function approximators, meaning that given a set of input-output pairs $(x,y)$, where $y= f(x)$, a NN $N$ can approximate $f$ with arbitrary precision.  

--

```{r, echo = FALSE}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/yfx.png")
```

Image taken from this [website](https://www.youtube.com/watch?v=bljuV9WDvAA) (minute 0:13).

---

## Introduction | Neural Networks | 2

.pull-left35[
```{r plot-first, out.width="100%", echo = FALSE}
library(neuralnet)
x <- seq(-1, 1, by = 0.05)
y <- x^2 + rnorm(length(x), mean = 0, sd = 0.01)
data <- data.frame(x, y)
model <- neuralnet(y ~ x, data = data, hidden = c(5)) #<<
new_data <- data.frame(x = seq(-1, 1, by = 0.01))
predictions <- compute(model, new_data)$net.result
par(family = "Palatino", cex = 1.5)
plot(x, x^2, type = "l", lwd = 2, col = "blue", 
     ylim = c(0, 1.2), xlab = bquote(x), ylab = bquote(y), 
     main = bquote("Aproximation of"~f(x) == x^2))
lines(new_data$x, predictions, col = "red", lwd = 2, lty = "dotted")
legend("topright", legend = c("f", "N"), col = c("blue", "red"), lwd = 2)
```
]
--
.pull-right65[
```{r ref.label = 'plot-first', eval = FALSE, echo = TRUE}
```
]

This is a 1 hidden layer NN with 5 neurons and sigmoid activation function.

---

## Introduction | Neural Networks | 3

.pull-left[
```{r, echo = FALSE}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/MLP.png")
```

Representation of a basic NN. Image source: `r Cite(myBib, "bishop2006pattern", before = "Pag. 228, Fig 5.1, ")`
]

.pull-right[
Let
- $\boldsymbol{x} = [x_0,x_1, \dots, x_D]^\top$
- $\boldsymbol{z} = [z_0, z_1, \dots, z_M]^\top$ 
- $\boldsymbol{y} = [y_0,y_1, \dots, y_K]^\top$
- $\boldsymbol{W}^{(1)}$ be the weight matrix connecting $\boldsymbol{x}$ and $\boldsymbol{z}$
- $\boldsymbol{W}^{(2)}$ be the weight matrix connecting $\boldsymbol{z}$ and $\boldsymbol{y}$

This way, $N$ can be written as 
\begin{equation}
    N(\boldsymbol{x}) = a_2(\boldsymbol{W}^{(2)}a_1(\boldsymbol{W}^{(1)}\boldsymbol{x})), 
\end{equation}
where $a_1$ and $a_2$ are activation functions.
]

---

## Introduction | Neural Networks | 3
- We can also represent $N$ as
\begin{equation}
\boldsymbol{x} \rightarrow \boldsymbol{z} = h_1(\boldsymbol{x})\rightarrow \boldsymbol{y} = h_2(\boldsymbol{z}),
\end{equation}
where $h_1(\cdot) = a_1(\boldsymbol{W}^{(1)}\cdot)$ and $h_2(\cdot) = a_2(\boldsymbol{W}^{(2)}\cdot)$.

--

- More generally, a NN $N$ with $L-1$ hidden layers can be represented as
\begin{equation}
\boldsymbol{x_0} \rightarrow \boldsymbol{x_1} = h_1(\boldsymbol{x_0})\rightarrow \dots \rightarrow \boldsymbol{x_L} = h_L(\boldsymbol{x_{L-1}}),
\end{equation}
where $a_\ell$ are activation functions and $h_\ell(\cdot) = a_\ell(\boldsymbol{W}^{(\ell)}\cdot)$.

--

- To avoid vanishing gradients, which is a problem that arises when $L$ is large, `r Citet(myBib, "HeResnet")` introduced Residual Networks (ResNets). They have the following representation
\begin{align*}
\boldsymbol{x_0} \rightarrow \boldsymbol{x_1} -\boldsymbol{x_0} = h_1(\boldsymbol{x_0})\rightarrow \dots \rightarrow \boldsymbol{x_L} - \boldsymbol{x_{L-1}} = h_L(\boldsymbol{x_{L-1}}).
\end{align*}

---
## Introduction | Neural ODE, `r Cite(myBib, "chen2018neural")` and `r Cite(myBib, "torchdiffeq")` | 1

- Observe that each layer has the form 
\begin{equation}
\boldsymbol{x_{\ell+1}} -\boldsymbol{x_\ell} = F(\boldsymbol{x_\ell})
\end{equation}
for some function $F$. Does this formula look familiar? 

--

- What about now?
\begin{equation}
\dfrac{X^{n+1}-X^n}{1} = F(X^n).
\end{equation}

--

- Exactly, this is forward Euler discretization of the derivative with step size 1.

--

- If we add more layers and take smaller step sizes, then, in the limit, we are actually approximating the IVP 

\begin{equation}
\boldsymbol{x}'(t) = F(\boldsymbol{x}(t)), \qquad \boldsymbol{x}(0) = \boldsymbol{x_0}
\end{equation}

--

- We can then use this by specifying the output layer $\boldsymbol{x_L}$ to be $\boldsymbol{x}(T)$, the solution of the IVP at some time $T$. 

- .content-box-blue[In practice, implementing this approach amounts to using an ODE solver during the NN's training.]
---

## Soving an ODE | finite difference approach | 1

We consider the following IVP

$$\begin{bmatrix}y_1'\\y_2'\\\end{bmatrix} =
\begin{bmatrix}-0.1&-2\\2&-0.1\\\end{bmatrix}
\begin{bmatrix}y_1^3\\y_2^3\\\end{bmatrix},
\qquad
\begin{bmatrix}y_1(0)\\y_2(0)\\\end{bmatrix}=
\begin{bmatrix}2\\0\\\end{bmatrix},\qquad t\in[0,25].\label{s}\tag{IVP}$$

The associated Jacobian matrix is given by 
$$\begin{bmatrix}-0.3y_1^2&-6y_2^2\\6y_1^2&-0.3y_2^2\\\end{bmatrix},$$ which has eigenvalues
\begin{equation}
    \lambda_{\pm} = \dfrac{3}{20}\left(-y_1^2-y_2^2\pm\sqrt{y_1^4-1602y_1^2y_2^2+y_2^4}\right)
\end{equation}
satisfying $-0.60038 \leq \lambda_{+}\leq 0$ and $-1.20000 \leq \lambda_{-}\leq 0$ for $y_1\in[-2,2]$ and $y_2\in[-2,2]$.

---
## Soving an ODE | finite difference approach | 2

Because the magnitude of these eigenvalues is relatively small, we expect that even the naive forward Euler's method will perform well. Here is its formulation.
\begin{align}
    Y_1^{n+1} &= Y_1^n +k\left(-0.1[Y_1^n]^3-2[Y_2^n]^3\right),\qquad Y_1^0  = 2,\tag{ForwardEuler}\\
    Y_2^{n+1} &= Y_2^n +k\left(-0.1[Y_2^n]^3+2[Y_1^n]^3\right),\qquad Y_2^0  = 0.\label{eulerm}\tag{ForwardEuler}
\end{align}

.pull-left35[
```{r, out.width = "85%", echo = FALSE}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/f2.png")
```
]

.pull-right65[
We consider three numerical methods to solve \eqref{s}.

- Solution .content-box-bluepython[BDF] was computed using [`scipy.integrate.solve_ivp(..., method = 'BDF'`)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html) from [SciPy](https://scipy.org/) library. 
- Solution .content-box-orangepython[odeint] was computed using [`odeint(..., method = 'dopri5')`](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/odeint.py) from [torchdiffeq](https://github.com/rtqichen/torchdiffeq) library. 
- Solution .content-box-greenpython[Euler] was computed using the forward Euler's method \eqref{eulerm}. 
- All three methods use 3052 integration points (equidistant in the cases of `odeint` and `Euler`).
]

---
## A note on abbreviations | 1

- [`BDF`](https://en.wikipedia.org/wiki/Backward_differentiation_formula#Specific_formulas): Implicit multi-step variable-order (1 to 5) method based on a backward differentiation formula.

$$U_{n+1}-U_n=k f\left(U_{n+1}\right)\tag{BDF1}$$
$$U_{n+2}-\frac{4}{3} U_{n+1}+\frac{1}{3} U_n=\frac{2}{3} k f\left(U_{n+2}\right)\tag{BDF2}$$
$$U_{n+3}-\frac{18}{11} U_{n+2}+\frac{9}{11} U_{n+1}-\frac{2}{11} U_n=\frac{6}{11} k f\left(U_{n+3}\right)\tag{BDF3}$$
$$U_{n+4}-\frac{48}{25} U_{n+3}+\frac{36}{25} U_{n+2}-\frac{16}{25} U_{n+1}+\frac{3}{25} U_n=\frac{12}{25} k f\left(U_{n+4}\right)\tag{BDF4}$$
$$U_{n+5}-\frac{300}{137} U_{n+4}+\frac{300}{137} U_{n+3}-\frac{200}{137} U_{n+2}+\frac{75}{137} U_{n+1}-\frac{12}{137} U_n=\frac{60}{137} k f\left(U_{n+5}\right)\tag{BDF5}$$
Formulas taken from this [website](https://en.wikipedia.org/wiki/Backward_differentiation_formula#Specific_formulas).

---
## A note on abbreviations | 2

- [`dopri5`](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/dopri5.py): Explicit (embedded) Runge-Kutta 4(5) of Dormand-Prince-Shampine (adaptive stepsize). Find below the Butcher tableau for this method.

```{r, out.width = "65%"}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/butcher45.png")
```

Image taken from this [website](https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods#Dormand%E2%80%93Prince).

---

## A note on learning | 1

Let $(\boldsymbol{X}, \boldsymbol{Y})$ be the training data and let us consider a NN $N$ given by $N(\boldsymbol{x}) = a_2(\boldsymbol{W}^{(2)}a_1(\boldsymbol{W}^{(1)}\boldsymbol{x}))$

1. **Data batching**. Divide the data into batches, say $(\boldsymbol{X}_i, \boldsymbol{Y}_i)$, where $\boldsymbol{X}_i\subset\boldsymbol{X}$ and $\boldsymbol{Y}_i\subset\boldsymbol{Y}$.

2. **Weight initialization**. Randomly initialize the weights $\boldsymbol{W}^{(1)}$ and $\boldsymbol{W}^{(2)}$. This is done just one time.

3. **Forward pass**. With the initialized weights $\boldsymbol{W}^{(1)}$ and $\boldsymbol{W}^{(2)}$, compute $N(\boldsymbol{X}_i)$.

4. **Loss computation**. Compute the loss function $\texttt{loss}(\text{parameters}) = \texttt{mean}|\boldsymbol{Y}_i-N(\boldsymbol{X}_i)|$. 

5. **Back-propagation**. Compute the gradients of the loss function with respect to the parameters $\boldsymbol{W}^{(1)}$ and $\boldsymbol{W}^{(2)}$ using the chain rule and back-propagate the error.

6. **Parameter update**. Update the weights $\boldsymbol{W}^{(1)}$ and $\boldsymbol{W}^{(2)}$ using the gradients computed in the back-propagation step.

7. **Iterate**. Repeat the forward pass, loss computation, back-propagation, and parameter update steps until the loss function converges.

---

## A note on learning | 2

.center[<iframe width="784" height="441" src="https://www.youtube-nocookie.com/embed/Tsvxx-GGlTg?si=nFSgaJk_BCW-M2tC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>]

Visit this [page](https://playground.tensorflow.org/) to play with a NN.

---
## ODEnet approach

We consider the following NN
.content-box-blue[
\begin{equation}
\label{nnf}\tag{N}
    N(\boldsymbol{u}) = \boldsymbol{W}^{(2)}\tanh(\boldsymbol{W}^{(1)}\boldsymbol{u}^3),
\end{equation}
]
where $\boldsymbol{u} = [u_1,u_2]^\top$, $\boldsymbol{W}^{(1)}\in\mathbb{R}^{50\times2}$, and $\boldsymbol{W}^{(2)}\in\mathbb{R}^{2\times50}$.

In practice, we proceed as follows. Let $\boldsymbol{t} = [t_0,\dots,t_M]^\top$ and $\boldsymbol{y}$ denote the given data and $\boldsymbol{y}_\text{*}$ be a random batch of $\boldsymbol{y}$. Let $\boldsymbol{t}_\circ = [t_0,\dots,t_K]^\top$ for some $K\leq M$. During the forward pass, we compute
\begin{align}
\label{nna}
    \hat{\boldsymbol{y}}_* = \texttt{odeint}(\texttt{func} = N,\; \texttt{y}_0 = \boldsymbol{y}_*,\; \texttt{t} = \boldsymbol{t}_\circ,\; \texttt{method} = \texttt{'method'})
\end{align}

- During back-propagation, we optimize $\texttt{loss} = \texttt{mean}|\boldsymbol{y}_*-\hat{\boldsymbol{y}}_*|$. 

- Observe that $\texttt{func} =N$, instead of the RHS of \eqref{s}, as usual. 

- This means that the NN $N$ is actually learning the dynamic function of the system. 

---

## test

.center[![](output.gif)]

---
class: center, middle
#### dopri8

---

layout: true

# dopri8

---


```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/000.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/001.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/002.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/003.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/004.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/005.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/006.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/007.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/008.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/009.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/010.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/011.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/012.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/013.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/014.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/015.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/016.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/017.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/018.png")
```

---

```{r}
knitr::include_graphics("~/Desktop/Spring 2024/AMCS252lenin/Learning/dopri8/019.png")
```

---

layout: false

These slides are based on the work of `r Cite(myBib, "xaringanpackage")`.
## References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib, .opts = list(bib.style = "alphabetic"))
```
